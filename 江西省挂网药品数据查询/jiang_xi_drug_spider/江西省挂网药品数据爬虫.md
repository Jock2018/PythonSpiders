## 声明

本爬虫仅供学习交流使用，请勿用作商业用途。爬取的时候注意控制速度，勿对网站造成攻击。

## 1. 实现说明
该政府网站没有任何反爬虫措施，直接抓取即可，注意不要对网站造成攻击。

思路：
1. 通过 scrapy 抓取网页；
2. 利用 xpath 解析网页，提取药品信息；
3. 将提取的信息写入 csv 文件；
4. 观察网页结构，构造下一页 url;
5. 新的 url 重复 1-4 步。

## 2. 一些细节说明

主要参考文档：

1. [Scrapy 官方文档](https://docs.scrapy.org/en/latest/intro/tutorial.html#creating-a-project)
2. [Scrapy 中 xpath 的使用](https://blog.csdn.net/qq_27283619/article/details/88704479)

使用的一些命令：

1. 创建一个 scrapy 项目：`scrapy startproject jiang_xi_drug_spider`
2. 在 shell 中使用 scrapy 爬取一个页面，便于调试抓取数据：`scrapy shell "https://yp.jxyycg.cn/djypcg/jxyycg/search/medicineRegionBidPriceQry.shtml?dypn=1"
'`

注：因为网站很规律，没有遇到异常， 所以代码没有做异常捕获处理。正常需要，捕获异常， 记录爬取失败的 url和数据，方便分析和重新抓取。


## 3. 一些踩坑说明
1. 存为 csv 时，用本地的 Excel 打开乱码，可以根据这篇帖子解决：[csv 文件打开乱码，有哪些方法可以解决？](https://www.zhihu.com/question/21869078)

## 4. 核心代码

```python
import os
from typing import Any

import scrapy
import csv
import pathlib


class DrugSpider(scrapy.Spider):
    name = 'jiang_xi_drug'
    start_urls = ['https://yp.jxyycg.cn/djypcg/jxyycg/search/medicineRegionBidPriceQry.shtml?dypn=1']
    data_path = pathlib.Path(os.path.abspath("." + os.sep + "jiang_xi_drug.csv"))
    title = ['目录ID',
             '标准通用名',
             '标准剂型',
             '标准规格',
             '包装单位',
             '生产企业',
             '地区',
             '包装数量',
             '最小制剂单位',
             '包装价格',
             '提交时间',
             '备注']

    def parse(self, response: Any) -> Any:
        if not self.data_path.is_file():
            with open(self.data_path.resolve(), "a+", encoding="utf-8", newline='') as file:
                # 第一次创建文件时，写入 title
                csv.writer(file).writerow(self.title)
        with open(self.data_path.resolve(), "a+", encoding="utf-8", newline='') as file:
            # 获取每页所有的词条
            for drug in response.xpath('//tr[@class="hover"]'):
                # 获取单个药品所有信息
                drug_message = drug.xpath('./td/text()').getall()
                csv.writer(file).writerow(drug_message)
        # 判断是否存在下一页
        if "下一页" in response.xpath('//a[@title]/text()').getall():
            base_url, index = response.url.rsplit("=", 1)
            next_page = base_url + "=" + str(int(index) + 1)
            yield scrapy.Request(next_page, callback=self.parse)

```

说明：开发调试时，可以直接通过 `execute_spider.py` 进行调试和执行该爬虫。
